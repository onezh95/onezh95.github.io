<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>CRF系列——一个简单的例子 | Zhaohui's blog</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">Zhaohui's blog</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick=""><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>CRF系列——一个简单的例子</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2019-06-02</div></div></div><article><div class="container post"><p>当我们需要对一串序列进行自动标注时，例如对文字序列进行词性标，判断每个单词的词性，每一个词的标注结果，不仅依赖于该词语本身，还会依赖于该序列中这个词前面的部分，CRF可以考虑到这样的依赖关系。</p>
<p>本位会以一个词性标注的应用为例，介绍CRF所解决的问题以及CRF模型的建立，学习与预测过程。</p>
<h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p><strong>词性标注</strong>（part-of-speech tagging），其目标就是对一个句子即一串单词的序列进行打标签（tagging），标注每个词的词性（ADJECTIVE, NOUN, PREPOSITION, VERB, ADVERB, ARTICLE）。我们假设在词性标准任务中，<strong>每个单词的词性不仅依赖其本身，而且也会依赖其前一个单词</strong>（这里做了简化，实际上每个单词的标注结果依赖更多东西）。</p>
<p>下面，我将针对对这个问题，建立一个简单的线性链条件随机场（linear-chain CRF）。并介绍这个CRF是如何表示上述的依赖关系，以及如何利用这个CRF来求解这个问题。</p>
<p>实际上，和其他的统计机器学习模型一样，我们有三个任务要完成：</p>
<p>1）指定模型参数（建立模型）</p>
<p>2）估计这些参数（学习）</p>
<p>3）利用这些参数进行预测（预测）</p>
<h2 id="第一个任务——建立模型"><a href="#第一个任务——建立模型" class="headerlink" title="第一个任务——建立模型"></a>第一个任务——建立模型</h2><h3 id="特征函数"><a href="#特征函数" class="headerlink" title="特征函数"></a>特征函数</h3><p>为了评价序列中每个单词属于各个tag的可能性，我们要根据单词的位置定义一系列<strong>特征函数</strong>（feature function），在线性链CRF模型中，特征函数表示如下：<br>$$<br>f_k(x,l_{i-1},l_{i},i), \qquad k\in{1,2,…,M}<br>$$<br>其中$s$表示序列，$i$表示该词在序列中的位置，$l_{i}$表示对该单词标注的词性。</p>
<p>每个特征函数表示的是当位置为$i$时，当前位置单词标注为$l_i$且它前一个单词标注为$l_{i-1}$时的”可能性”，不过这个”可能性”并不是一个概率，它通常为0或1。</p>
<p>例如，可以定义如下特征函数：</p>
<p>$f_1(x,l_{i-1},l_{i},i)=\cases{1,\qquad l_i=ADVERB\ \&amp;\ 第i个单词以‘-ly’结尾\0,\qquad otherwise}$</p>
<p>$f_2(x,l_{i-1},l_{i},i)=\cases{1,\qquad i=1\ \&amp;\ l_i=VERB\ \&amp;\ 句子以问号结束\0,\qquad otherwise}$</p>
<p>$f_3(x,l_{i-1},l_{i},i)=\cases{1,\qquad l_{i-1}=ADJECTIVE\ \&amp;\ l_i=NOUN\0,\qquad otherwise}$</p>
<p>$f_4(x,l_{i-1},l_{i},i)=\cases{1,\qquad l_{i-1}=PREPERSITION\ \&amp;\ l_i=PREPERSITION\0,\qquad otherwise}$</p>
<p>…</p>
<p>可以看到每一个特征函数的值只与当前位置和前一个位置有关，因此，可以说它有能力表示出我们之前所说的依赖关系。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>有了特征函数，我们就可以开始建立模型了。</p>
<p>所谓的建模，其实就是要表示出当给定句子，也就是单词序列$x={x_1x_2…x_N}$时，整个句子标注为$l=(l_1l_2…l_N)$的概率，即条件概率：<br>$$<br>P(L=l_1l_2…l_N|X={x_1x_2…x_N})<br>$$<br>也就是当$X$给定的情况下，$l_1,l_2,…,l_N$的联合概率<br>$$<br>P(L_1=l_1,L2=l_2…L_N=l_N|X={x_1x_2…x_N})<br>$$<br><strong>权重</strong></p>
<p>每个特征函数对整体的影响应该是不同的，因此，需要为每个函数加上<strong>权重</strong>$W=(w_1,w_2,…,w_M)$，权重越大，则该特征函数对标注结果的影响越大。那么每个特征函数的权重应该取多少呢？实际上这些权重$W$就是我们所说的参数了，而这些参数如何取值就是我们的第二个任务——<strong>参数学习</strong>，这个会在下一节讲。</p>
<p><strong>非归一化概率</strong></p>
<p>每个特征函数表示的是每个位置的可能性，那么整个句子被标注为$l=(1_1l_2…l_N)​$的可能性当然就是各个位置的可能性的乘积。<br>$$<br>整个句子被标注为l的可能性=\prod_{k=1}^{M}\prod_{i=1}^{N}w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1})<br>$$<br>不过为了方便计算，我们使用<strong>指数形式</strong>来表示特征：$exp(w_kf_k(s,l_{i-1},l_i,i))​$。这样，我们的乘法就变成了加法：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>整个句子被标注为l的可能性<br>&amp;=\prod_{k=1}^{M}\prod_{i=1}^{N}exp(w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1}))\<br>&amp;=exp(\sum_{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1}))<br>\end{aligned}<br>\end{equation}<br>$$<br><strong>参数模型</strong></p>
<p>此时，我们的模型已经呼之欲出了，</p>
<p>现在所表示的”可能性”还不是一个概率，应该要做归一化处理，使它的值在0到1之间，因此引入归一化项$Z(x)$：<br>$$<br>Z(x)=\sum_{L=l’} exp(\sum_{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l’_{i-1},L_i=l’_{i+1}))<br>$$</p>
<p>这样，我们所需要的条件概率就得到了：<br>$$<br>P(L=l|X=x)=\frac{1}{Z(x)}exp(\sum_{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1}))<br>$$<br>其中$l​$与$x​$都是向量，$l=l_1l_2…l_N,x=x_1x_2…x_N​$ </p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这就是我们的线性链CRF参数模型，它有能力表示这样的依赖关系：每个位置的标注结果不仅依赖于该位置的单词本身，同时还依赖前一个位置的标注结果。</p>
<p>同时我们还指定了模型的参数为每个特征函数的权重$W=(w_1,w_2,…,w_M)$。</p>
<h2 id="第二个任务——学习"><a href="#第二个任务——学习" class="headerlink" title="第二个任务——学习"></a>第二个任务——学习</h2><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>学习的<strong>目标</strong>其实就是找到一组参数$w=(w_1,w_2,…,w_M)​$，在训练数据序列$x=(x_1,x_2,…,x_N)​$和标注$l=(l_1,l_2,…,l_N)​$上，使条件概率$$P(L=l|X=x)​$$最大化，即找到$w^<em>​$,使：<br>$$<br>w^</em>=\mathop{\arg\max}_w(P(l|x))<br>$$<br>学习的方法有很多，例如梯度下降，最大似然估计等，这里就不多说了。</p>
<p>这样学习的任务看似就完成了，不过等等！问题好像并不是这么简单。</p>
<h3 id="计算问题"><a href="#计算问题" class="headerlink" title="计算问题"></a>计算问题</h3><p>我们再看看$Z(x)$：<br>$$<br>Z(x)=\sum_{L=l’} exp(\sum_{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l’_{i-1},L_i=l’<em>{i+1}))<br>$$<br>看到最左边的求和：$\sum</em>{L=l’}​$，这里其实写得简单了，$L​$是一个向量，所以其完整形式应该是：<br>$$<br>\sum_{L_1=q_1}^{Q}\sum_{L_2=q_1}^{Q}…\sum_{L_N=q_1}^{Q}, \qquad Q={q_1,q_2,…,q_m}<br>$$<br>即在每个位置上都有$m$种可能的标注结果，那么长度为$N$的句子，就有$m^N$种情况，即使每个单词只分两种词性，长度为15个单词的序列，其计算量都有$2^{15}$，因此<strong>想要直接计算$Z(x)$是很难的</strong>。</p>
<p><strong>前向-后向算法</strong></p>
<p>我们所研究的对象，通常都是很长的序列，根据上面所说，对于这种高维数据，想要直接计算$Z(x)$是很难的，因此，在线性链CRF中，为了得到$Z(x)$需要<strong>前向-后向算法</strong>。</p>
<p>对于前向-后向算法，在这里先不做过多说明，在之后的文章中会详细介绍。</p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>这样，我们第二个任务也搞定了。</p>
<p>参数的学习就是根据训练数据找到能使条件概率最大化的参数；</p>
<p>但是由于数据维度高，计算量很大，因此会使用前向-后向算法来计算线性链条件随机场的归一化项$Z(x)$。</p>
<h2 id="第三个任务——预测"><a href="#第三个任务——预测" class="headerlink" title="第三个任务——预测"></a>第三个任务——预测</h2><p>现在我们已经有了模型，并且在已知的数据集上得到了该模型最优的参数，接下来就是根据这个模型及参数对未知的数据进行推测了。具体到我们的词性标注问题，就是给定一个已知句子$x$，但是其标注未知，通过模型推测每个单词可能性最大的词性，即找到最优的一组$l^<em>$，使：<br>$$<br>l^</em>=\mathop{\arg\max}_l(P(L=l|X=x))<br>$$</p>
<h3 id="计算问题-1"><a href="#计算问题-1" class="headerlink" title="计算问题"></a>计算问题</h3><p>这回我们谨慎一点，先看看计算有没有问题。</p>
<p>把公式展开：<br>$$<br>\begin{aligned}<br>l^<em>&amp;=\mathop{\arg\max}_lP(L=l|X=x)\<br>&amp;=\mathop{\arg\max}<em>l\frac{1}{Z(x)}exp(\sum</em>{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1}))<br>\end{aligned}<br>$$<br>我们只是要找到能使$P(L=l|X=x)$概率最大的一个$l$，而对于每一个$l$，$Z(x)$都是一样的，因此原问题变成：<br>$$<br>\begin{aligned}<br>l^</em><br>&amp;=\mathop{\arg\max}<em>lexp(\sum</em>{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1}))\<br>&amp;=\mathop{\arg\max}<em>l(\sum</em>{k=1}^{M}\sum_{i=1}^{N}w_kf_k(x,L_{i-1}=l_{i-1},L_i=l_{i+1}))<br>\end{aligned}<br>$$<br>太好了，$Z(x)$不用计算了。</p>
<p>不过为了找到最优的$l^{*}$，总不可能真的把每一个$l$都试一遍吧，这样计算量仍然很大。</p>
<p>那么<strong>维特比算法</strong>就是为了解决上述问题的，它利用了动态规划的思想，不过具体算法我们也先暂时不说，会在后面的文章里详细说明。</p>
<h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>这样，我们知道了如何利用建立好的模型以及学习好的参数，对词性未知的句子进行标注了。同时，为了得到这个最优的标注，需要使用维特比算法</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至此，我们通过一个词性标注的例子，了解了：</p>
<ol>
<li>CRF适用于这样的情况：对于多个位置的标注，每一个位置的标注结果，不仅与其本身有关，而且依赖于其他位置的标注结果。</li>
<li>CRF与其他统计机器学习模型一样，可以分为三个部分，分别是：<ol>
<li>模型的建立</li>
<li>参数的学习</li>
<li>推测</li>
</ol>
</li>
<li>对于模型的建立，我们使用了特征函数，它保证了模型拥有表示我们所需的依赖关系的能力。</li>
<li>对于模型参数的学习以及结果的预测，发现高维数据带来很大的计算量，因此需要一些算法来简化计算。</li>
</ol>
<p>但是，还有一些问题需要探索：</p>
<ol>
<li>CRF是一种概率图模型，那么什么是概率图模型？从概率图的角度我们怎么看条件随机场？其他的概率图模型又能解决什么样的问题？</li>
<li>我们在这里说的都是线性链条件随机场(Linear Chain CRF)，那么其他形式的CRF是怎样的，它们能解决什么问题？</li>
<li>在线性链CRF中，为了解决计算问题，在学习和预测中所用到的前向-后向算法以及维特比算法分别是什么？</li>
<li>在图像处理中，如何利用CRF提高预测效果？</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/" target="_blank" rel="noopener">introduction to conditional random fields</a></p>
<p>[2] <a href="https://www.jiqizhixin.com/articles/2018-05-15-9" target="_blank" rel="noopener">如何直观地理解条件随机场，并通过PyTorch简单地实现</a></p>
<p>[3] <a href="https://zhuanlan.zhihu.com/p/22464581" target="_blank" rel="noopener">FCN(2)——CRF通俗非严谨的入门</a></p>
<p>[4] 李航. 统计学习方法[M]. 2012.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:wangzh95@mail.ustc.edu.cn" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/onezh95" target="_blank"><i class="fa fa-github"></i></a><a href="/atom.xml" target="_blank"><i class="fa fa-rss"></i></a></div><div class="footer">© 2019 <a href="/" rel="nofollow">Zhaohui</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>